[1] Design DB schema and generate SQL code use dbdiagram.io

[2] Install & use Docker + Postgres + TablePlus to create DB schema

[3] Write & run database migration in Golang
  - install golang migrate : 
    wget http://github.com/golang-migrate/migrate/releases/latest/download/migrate.linux-amd64.deb
    sudo dpkg -i migrate.linux-amd64.deb

  - init db schema with name init_schema:
    migrate create -ext sql -dir db/migration -seq init_schema

  - write up and down schema :

  - create database directly use docker exec :
    sudo docker exec -it pgsql createdb --username=tech_school --owner=tech_school simple_bank

  - to delete : 
    sudo docker exec -it pgsql createdb --username=tech_school --owner=tech_school simple_bank 

  - create Makefile and fill it :

  - run migrate up:
    migrate -path db/migration -database "postgresql://tech_school:21204444@localhost:5432/simple_bank?sslmode=disable" -verbose up # -verbose=" ask migrate to print verbose logging"

[4] Generate CRUD Golang code from SQL | Compare db/sql, gorm, sqlx & sqlc
  - menggunakan SQLC :
    docker pull sqlc/sqlc # pastikan arsitekturnya sesuai contoh (linux/amd64)

  - run for init file sqlc.yaml :
    sudo docker  run --rm -v $(pwd):/src -w /src sqlc/sqlc:1.8.0 init

  - write generated sqlc.yaml :

  - write query code for sqlc to use as code base :
    reference : https://docs.sqlc.dev/en/stable/tutorials/getting-started-postgresql.html
  
  - run sqlc to generate code :
    sudo docker run --rm -v $(makeFileDir):/src -w /src sqlc/sqlc:1.8.0 generate
  
[5] Write Golang unit tests for database CRUD with random data
  - setup connection to db in main_test.go :
    DB functions are defined as method of Queries object therefore we need to setup db connection to talk to the db
  
  - create random utils :

[6] A clean way to implement database transaction in Golang
  # db transaction is single unit of work, often made up of multiple db operations. see : 06 transfer example.png
  # why do we need db transaction :
    # - to provide a reliable and consistent unit of work, even in case of system failure
    # - to provide isolation between programs that acccess the database concurrently

  # untuk memenuhi 2 tujuan diatas maka database harus memenuhi ACID PROPERTY
    # - Atomicity  = sebuah operasi harus berhasil sepenuhnya atau  gagal sepenuhnya dan database berubah kembali ke kondisi awalnya (unchanged) jika gagal
    # - Consistentcy = keadaan database harus valid setelah transaction. semua constraint (batasan atau syarat") harus terpenuhi. atau dengan kata lain semua data yang diinputkan harus valid sesuai dengan tipe datanya termasuk cascade (sequence of some event)
    # - isolation = transaction yang berjalan bersamaan (concurrently) tidak boleh mempengaruhi satu sama lain. ada  beberapa level dari isolation yang mendefinisikan kapan suatu perubahan yang dilakukan oleh sebuah transaction dapat dilihat oleh transaction lainnya
    # - durability = data data transaction yang sukses dilakukan harus terecord dan terus menerus harus ada didalam penyimpanan atau tidak boleh hilang apapun yang terjadi pada sistem
    
  - create transaction in store.go in db/sqlc :

  - create test for store.go :

[7] DB transaction lock & how to handle deadlock in Golang
  # jika kita tidak menerapkan isolation maka akan ada kamungkinan terjadinya "read phenomena"

  #  READ PHENOMENA
  #  terjadi jika ada satu transaction yang terpengaruhi dengan transaction lainnya
  #      a. when low level transaction isolation

  #      - dirty read 
  #      = a transaction READS data writen by other concurrent UNCOMMITED transaction. ini buruk karna kita tidak tau apakah data yang dibaca tersebut pada akhirnya akan di commit atau akan di rollback

  #      - non-repeatable read 
  #     = a transaction READS the SAME ROW TWICES and return different value because it has been        MODIFIED after the first read by other COMMITED transaction. 

  #      - phantom read 
  #      = a transaction RE-EXEECUTES a query to FIND ROWS that satisfy a condition and return a DIFFERENT SET of rows, due to changes by other recently commited transaction. pada pemanggilan pertama ada 5 row lalu ketika dipanggil lagi hanya 4 yang muncul
        
  #      - serialization anomaly
  #      = the result of a GROUP of concurrent COMMITED TRANSACTION is IMPOSIBLE TO ACHIVE if we try to run them SEQUENTIALLY in any order without overlapping


  #  4 STANDARD ISOLATION LEVELS (by ANSI (american national standards institue))

  #  makin tinggi nomornya berarti semakin tinggi
  #  1. read uncommited = pada level ini sebuah transaksi dapat membaca data dari transaksi lain yang belum di commit

  #  2. read commited = tranksaksi hanya bisa membaca data dari tranksaksi lain yang sudah di commit

  #  3. repeatable read = same read or select query always return same result. tidak peduli berapa kali dieksekusinya bahkan ketika ada transaksi lain yang melakukan commit, jika ada commit dari tranksaksi lain maka akan mengembalikan nilai lama seakan transaksi lain tsb tidak pernah terjadi

  #  4. serializable = can achive same result if execute transaction serially in some order instead of concurrently. tranksaksi yang dijalankan secara concurent yang berjalan pada level ini dijamin menghasilkan hasil akhir yang sama jika mereka di eksekusi SEQUENTIALLY dalam urutan tertentu satu demi satu tanpa overlapping. 
  #  basically at least ada satu urutan untuk menjalankan concurent transaction sehingga ketika kita menjalankannya satu demi satu hasil akhirnya akan sama



  #  postgress ql

  #  read uncommited = untuk postgress tidak memiliki read uncommited, bisa kita set namun behaviournya sama seperti read commited

  #  read commited = mencegah dirty read

  #  repeatable read = mencegah non-repeatable read dan juga mencegah phantom read terjadi tetapi juga mencegah confusing state dimana pada sql lain meskipun kita tidak dapat membaca data yang diubah transaksi lain tapi kita masih bisa mengubah data di tranksaksi kita sendiri yang mana itu akan membuat kebingungan

  #  serializable = mencegah serializable anomaly dengan mencegah transaction read/write yang dilakukan tranksaksi lainnya yang mana transaction read/write tersebut nilainya bergantung dengan nilai yang terpengaruhi oleh tranksaksi yang lainnya lagi

  #  KEEP IN MIND when using high level isolation
  #  retry mechanism
  #  - there might be error, timeout or deadlock

  #  read documentation
  #  - each database engine might implement isolation level differently

  - add special sql to update an account for transaction :
   
    # -- name: GetAccountForUpdate :one
      SELECT * FROM accounts
      WHERE id = $1 LIMIT 1
      FOR NO KEY UPDATE; 

    # FOR UPDATE memastikan bahwa ketika ada satu transaksi berjalan maka transaksi lainnya harus menunggu transaksi itu selesai agar tidak terjadi dirty read

     #NO KEY setting memberitahu sql bahwa jika kita ingin melakukan transaction maka yang berubah hanya balance nya saja dan tidak merubah primary key yang terdapat constraint nya. hal ini berguna untuk melakukan transcation secara concurent untuk mencegah terjadinya deadlock.

  - add special queries to add an account balance directly is other way to prevent deadlock :


[8] How to avoid deadlock in DB transaction ? Queries order matters!
  # best way to deal with deadlock is to avoid it. by that it's mean we have to fine-tune our queries in the transaction so that deadlock won't have a chance to occur, or at least minimize its chance of occurence.

  # salah satu hal yang membuat deadlock terjadi adalah urutan eksekusi transaksi yang tidak terurut.
  # oleh karena itu salah satu cara untuk mengatasinya adalah dengan mengeksekusi transaksi secara beraturan.

  # dimana jika ada urutan eksekusi transaksi sql seperti berikut :
   # TRANSAKSI 1
      # UPDATE accounts SET balance = balance - 10 WHERE id = 1 RETURNING *; 1
      # UPDATE accounts SET balance = balance + 10 WHERE id = 2 RETURNING *; 3

    # TRANSAKSI 2
      # UPDATE accounts SET balance = balance - 10 WHERE id = 2 RETURNING *; 2
      # UPDATE accounts SET balance = balance + 10 WHERE id = 1 RETURNING *; 4
    
   # bisa diubah menjadi sperti berikut, agar eksekusinya urut sesuai ID akun yang diubah :
   # TRANSAKSI 1
      # UPDATE accounts SET balance = balance - 10 WHERE id = 1 RETURNING *; 1
      # UPDATE accounts SET balance = balance + 10 WHERE id = 2 RETURNING *; 3

    # TRANSAKSI 2
      # UPDATE accounts SET balance = balance + 10 WHERE id = 1 RETURNING *; 2
      # UPDATE accounts SET balance = balance - 10 WHERE id = 2 RETURNING *; 4


  - update transaction TransferTx code is other way to minimize deadlock :
    # kita bisa mengakali kode transaction TransferTX agar transaksinya dieksekusi seperti diatas. bisa dilihat di TransferTXV2

[9] Understand isolation levels & read phenomena in MySQL & PostgreSQL 
  # Isolation is one of the four property of a database transaction, where at its highest level, a perfect isolation ensures that all concurrent transactions will not afferct each other.
    
  # jika kita tidak menerapkan isolation maka akan ada kamungkinan terjadinya "read phenomena"

  # READ PHENOMENA
  # read phenomena terjadi jika ada satu transaction yang terpengaruhi dengan transaction lainnya
     #  a. when low level transaction isolation

      # - dirty read 
      # = a transaction READS data writen by other concurrent UNCOMMITED transaction. ini buruk karna # # kita tidak tau apakah data yang dibaca tersebut pada akhirnya akan di commit atau akan di rollback

      # - non-repeatable read 
      # = a transaction READS the SAME ROW TWICES and return different value because it has been MODIFIED after the first read by other COMMITED transaction. 

      # - phantom read 
      # = a transaction RE-EXEECUTES a query to FIND ROWS that satisfy a condition and return a DIFFERENT SET of rows, due to changes by other recently commited transaction. pada pemanggilan pertama ada 5 row lalu ketika dipanggil lagi hanya 4 yang muncul
      
      # - serialization anomaly
      # = the result of a GROUP of concurrent COMMITED TRANSACTION is IMPOSIBLE TO ACHIVE if we try to run them SEQUENTIALLY in any order without overlapping


  # 4 STANDARD ISOLATION LEVELS (by ANSI (american national standards institue))

  # makin tinggi nomornya berarti semakin tinggi
  # 1. read uncommited = pada level ini sebuah transaksi dapat membaca data dari transaksi lain yang belum di commit 

  # 2. read commited = tranksaksi hanya bisa membaca data dari tranksaksi lain yang sudah di commit

  # 3. repeatable read = same read or select query always return same result. tidak peduli berapa kali dieksekusinya bahkan ketika ada transaksi lain yang melakukan commit, jika ada commit dari tranksaksi lain maka akan mengembalikan nilai lama seakan transaksi lain tsb tidak pernah terjadi

  # 4. serializable = can achive same result if execute transaction serially in some order instead of concurrently. tranksaksi yang dijalankan secara concurent yang berjalan pada level ini dijamin menghasilkan hasil akhir yang sama jika mereka di eksekusi SEQUENTIALLY dalam urutan tertentu satu demi satu tanpa overlapping. 
  # basically at least ada satu urutan untuk menjalankan concurent transaction sehingga ketika kita menjalankannya satu demi satu hasil akhirnya akan sama



  # postgress ql

  # read uncommited = untuk postgress tidak memiliki read uncommited, bisa kita set namun behaviournya sama seperti read commited

  # read commited = mencegah dirty read

  # repeatable read = mencegah non-repeatable read dan juga mencegah phantom read terjadi tetapi juga mencegah confusing state dimana pada sql lain meskipun kita tidak dapat membaca data yang diubah transaksi lain tapi kita masih bisa mengubah data di tranksaksi kita sendiri yang mana itu akan membuat kebingungan

  # serializable = mencegah serializable anomaly dengan mencegah transaction read/write yang dilakukan tranksaksi lainnya yang mana transaction read/write tersebut nilainya bergantung dengan nilai yang terpengaruhi oleh tranksaksi yang lainnya lagi

  # KEEP IN MIND when using high level isolation
  # retry mechanism
  # - there might be error, timeout or deadlock

  # read documentation
  # - each database engine might implement isolation level differently

[10] Setup github actions for golang + postgres to run automated test

  - create and push to github repository:

  - set recommended workflows (prefered copy the generated content and doing it locally and push it later) :
    - go workflows 
      # copy dari go workflows buaat ci.yaml di github workflows, lalu rename build job jadi test job

  - add ci.yaml file to github workflows folder:

  - define Workflow :
    # workflow is basically an automated procedure that's made up of one or more jobs.
    # can be trigerred by : event, schedule, manually use button.
  

  # RUNNER = in order to run the jobs, we must specify a runner for each of them. a runner is simply a server that listens for available jobs. and only run one job at a time.

  # JOB = is a set of steps execute on the same runner. normal job run in parallel. dependent job run serially.

  # STEP = are individual task run serially within a job. a step contain 1 or multiple actions.

  #  ACTION = is standalone command. run serially within a step. can be reused

  - push to github repository  and see the test running:

[11] Implement HTTP API in Go using Gin
  - install gin:
    $ go get -u github.com/gin-gonic/gin
  
  - setup server in api/server.go :
  
  - create handler (account.go, entry.go, etc) :
    urutan kerja handler 
        - binding request (return error if exist)
        - run sql (return error if exist)
        - return response

  - create entry point for server in the main.go file :

  - add new makefile command to run server :
    go run main.go

[12] Load config from file & env vars with viper
  # reason to read vars from file
    # - DEVELOPMENT = easily specify default congfiguration for local development and testing

  # reason to read vars from env vars
    # - DEPLOYMENT = easyly override the default configurations when deploy with docker containers.

  # reason to use Viper to read vars 
    # able to find, load, unmarshal config file from JSON, TOML, YAML, ENV, INI
    # able to read config from environtment variables or flags. this will override existing values and set default values
    # able to read config from remote system such as Etcd, Consul
    # it works for both unencrypted and encrypted values.
    
    # viper able to watch for changes in the config file, and notify the application about it.
      # Live watching ad writing config file
        # reread changed file, save any modifications.

  - installing viper :
      go get github.com/spf13/viper
    
  - create and fill app.env :
    # to store our config values for development

  - create and define new utils to load config object, config.go :

  - implement on main.go, main_test.go :

  - try to user viper to read env var. run :
    SERVER_ADDRESS=0.0.0.0:8081 make server

[13] Mock DB for testing HTTP API in Go and achieve 100% coverage
  # why mock db ?
      # - Independent test = isolate test data from real database to avoid conflicts
          # because each test use it's own separate mock db to store data

      # - Faster test = reduce a lot of time talking to the database
          #  reduce  a lot of time talking to the database. karna test tidak harus menghabiskan banyak waktu talking to the db dan menunggu query untuk berjalan. semua action akan di lakukan di memory dalam proses yang sama

      # - 100% coverage = easily setup edge cases = unexpected errors
          # it allow us to write test that achive 100% coverage. with mock db, we can easily setup and test some edge cases such as unexpected error, or connection lost.

      # apakah sudah cukup jika kita menggunakan mock ? 
        #ya karena database kita yang asli sudah kita pernah tes sebelumnya

      # all we need to do is make sure that the mock db implement the same interface as the real db. then everything will be working just fine when being put together.

  - install gomock :
      go install go.uber.org/mock/mockgen@latest
  
  - add go/bin that store gomock package to path :
      add to vim ~/.zshrc ==> export PATH=$PATH:~/Desktop/go/bin

  - run :
      go get go.uber.org/mock/mockgen/model
    
  - update server.go and store.go code :
  
  - update sqlc config to emmit interface that contain all function of the queries struct :
    # querier.go will appear in db/sqlc after run generate

  - run mockgen :
      mockgen -package mockdb -destination db/mock/store.go github.com/tech_school/simple_bank/db/sqlc Store

  - start write test for api (account_test.go, etc) :

  - add new main_test in api :

[14] implement transfer money API with a custom params validator
  - create handler function in api (transfer.go) :

  - apply handler to router in server :

  - create new util, for currency :

  - create custom validator (validator.go) :

  - register custom validator to gin in server :

  - apply custom validator to binding request :
  

[15] add users table with unique & foreign key constraints in PostgreSQL for user authen and author
  - add user table and connect it with the existing accounts table via some db constraints :

  - update dbdiagram schema and export it as psql :

  - create new migration version :
    migrate create -ext sql -dir db/migration -seq v2add_users

  - define migrate up schema :

  - run make migrate up to apply newer migration:
   # ini mungkin akan error, karena kita sudah punya list akun sebelumnya sudah ada namun tidak terkait dengan owner manapun. error constraint

   # error ini membuat schmea migrations table pada db kita berada pada kondisi dirty, hal ini membuat kita tidak dapat menjalankan migrate down. untuk memperbaikinya kita hanya perlu secara manual mengubah nilai dirty menjadi false. setelah itu kita bisa melakukan mig down
  
      - lakukan migrate down :
      - dan jalankan migrate up lagi : 
        # kini sudah ada table user yang baru ditambahkan

  - define migrate down schema :
    # when writing the migration down, we should reverse what were done in the migration up.

  - add command in Makefile to run migrate down one version bellow :

  - add command in Makefile to run migrate up one version above :

[16] Handler DB errors in Golang correctly

  - create new SQL code for user table :

  - run make sqlcgenerate to update querier :

  - create testing for new user query function (user_test.go):

  - run make mock to update mock :

  - update previous query function testing code to support new user table and it's new constraint toward other table :

[17] Securely store passwords with Hash password in Go with Bcrypt
  # hashed password bertujuan agar password yang disimpan dalam database tidak disimpan begitu saja dalam bentuk password yang sesungguhnya. hal ini untuk mencegah orang" internal yang dapat mengakses database tidak bisa mengetahui password yang disimpan oleh pengguna.
  # jika ada yang meretas database berisi akun dan password yang disimpan oleh pengguna ia tidak bisa langsung menggunakan data" password yang ia dapatkan. 
  - create new utils for password (passwor.go):

  - create test for password util to check if it works properly :

  - update testing for generated query function to apply this hash password technique :

  - create API handler to handle create user that implement this hash password technique to manage user password :

  - register the handler to the route in server :

[18] How to write stronger  unit tests with a custom gomock matcher
  -

[19] Token based authentication 
    # menerapkan Token Based Authentication
    #  alur TBA 
    #    1. client melakukan POST untuk login (username,password)
    #    2. server akan mengecek apakah username dan password tsb benar apa tidak
    #    3. jika benar 
    #    4. server akan membuat dan sign a token 
    #    5. server mengirim status 200 dan juga access tokennya (jwt atau paseto) 
    #    6. token tsb dapat digunakan client ketika melakukan fetch dimana akan diletakan diheader fetch tsb
    #    7. contoh client request list akun bank
    #    8. server akan memverifikasi apakah token yang dikirim oleh client valid atau tidak 
    #    9. jika valid maka server akan mengirim balik status 200 beserta list akun yg di request 

[20] Create and verify JWT & PASETO in golang :
  - create new package (token) :

  - create payload struct and function to create and verify it :
    # membuat struct Payload berisi data payload yang terkandung didalam token
      - install UUID package for payload uuid :
        go get github.com/google/uuid

  - create token maker :
    # declare a general token maker interface to manage creation and verification of the tokens.

  - create JWT maker :
      - run to install golang JWT package :
        go get github.com/dgrijalva/jwt-go
      
      - create test function for JWT maker :

  - create PASETO :
      - run to install PASETO :
          go get -u github.com/o1egl/paseto

      - crate paseto maker :

      - create test function for PASETO maker to make sure it works properly :

[21]  Implement login user API that returns PASETO or JWT access token in Go
  - add token symetric key and access token duration to env file and config :
    # pass to server through newserver parameter

  - add newPasetoMaker to server API (server.go) :

  - add newTestServer in main_test in api to use in testing later:

  - update testing api to new NewServer parameter for config :

  - add new handler API for user login that implement token based authentication and also hash password :

  - register new handler to route in server :

  - because NewServer is getting more complicated now, it's better to refactor this function :

[22] Implement authentication middleware and authorization rules in golang gin 
  - create  authentication middleware for api :

      - write an unit test :

      - update CreateToken functions to return payload for test purpose :
          - update test function related to this CreateToken update :
          - update user handler api :
  
  - add midleware to ther server in setup router function :

  - add authorization for each specific API handler (lihat : go-backend-auth) :
      #  a logged-in user hanya bisa membuat akun untuk dirinya sendiri
      #  a logged-in user hanya bisa mendapatkan akun yang merupakan punyanya sendir
      #  a logged-in user hanya bisa mendapatkan list akun yang merupakan punyanya sendiri
      #  a logged-in user hanya bisa transfer uang dari dan miliknya sendir

    - update query function to get and list only items that belong to user in token payload :
      - run make sqlc :
      - run make mock :
      - update sqlc testing to adapt with new query function update : 


  - update api testing function / mock to adapt with new auth middleware :

[23] Build a minimal Golang docker image with a multistage Dockerfile
  # never push directly into master branch
  # new feature need separate branch from master and only merge it back after the new code is properly tested and reviewed

  - create new branch :
      git checkout -b ft/docker

  - create new Dockerfile to create golang backend docker image :

  - run to build :
      sudo docker build --tag simplebank:gin.http.1.0 .

[24] connect containers in the same docker network 
  

  - create network to connect golang backend contaner with postgress container :
    sudo docker network create simpe-bank-network

  - connect container to network :
    sudo docker network connect simple-bank-network pgsql
    sudo docker network connect simple-bank-network simplebank

  - create and run golang backend container :
    sudo docker container run --name simplebank --network simple-bank-network -p 8080:8080 -e DB_SOURCE="postgresql://tech_school:21204444@pgsql:5432/simple_bank?sslmode=disable" simplebank:gin.http.1.0

    for running in release mode add -e GIN_MODE=release :
        sudo docker container run --name simplebank --network simple-bank-network -p 8080:8080 -e GIN_MODE=release -e DB_SOURCE="postgresql://tech_school:21204444@pgsql:5432/simple_bank?sslmode=disable" simplebank:gin.http.1.0

  - update Makefile on postgress comman to immediately connect with docker network. add --network simple-bank-network :
        createpgcontainer: sudo docker container create --network simple-bank-network --name pgsql -p 5432:5432 -e POSTGRES_USER=tech_school -e POSTGRES_PASSWORD=21204444 postgres:15.4-alpine3.18

[48] run DB migrations directly inside Golang code
  - install golang migrate package :
      go get github.com/golang-migrate/migrate/v4
  
  - add db migration url to env and also update config tool struct:

  - update main.go to run db migration after config loaded :

  

[25] write docker compose file and  
  - setup wait.sh to make golang backend container wait postgres container to complete :
      # source : https://github.com/eficode/wait-for
       - download wait-for file and rename it wait-for.sh :
       - make it executable :
          chmod +x wait-for.sh
      
  - copy wait-for.sh file to final docker image in dockerfile :

  - add entrypoint in dockerfile :

  # dengan menggunakan compose kita tidak perlu setup network lagi karena container yang dihasilkan otomatis dibuatkan dan container akan ditempatkan pada network yang sama
  - write docker-compose.yaml :

  - run docker compose up :

[1.1] continue writing test
  - write new testing
  - make new mock
    - install :
      github.com/jackc/pgx/v5

  - update createUser API handler

[26] Create a free tier AWS account to deploy application

[27] Auto build & push docker image to AWS ECR with github Actions (PAUSED)
  - create a repository in AWS ECR to store our docker images :
    # ECR is a fuly-managed docker container registry that make it easy to store, manage, and deploy docker container images.

  - rename existing workflow ci.yml to test.yml  :

  - setup IAM :
    # Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources.
    - add provider :
        url : token.actions.githubusercontent.com
        audience : sts.amazonaws.com

    - create roles :
        

  - create new workflow (deploy.yml) :
      # we will use this workflow to build docker image and later deploy it to production


[37] manage user session with refresh token - Golang

    - add new env var to store Refresh token duration, and update config.file :

    - run new migration to create new scheme for new session table in the database :
        migrate create -ext sql -dir db/migration -seq add_sessions
    
    - define new session table up and down schema :

    - run migrate up :

    - add new SQL queries to create and retrive a session. :

    - run make sqlc  :
        re run make mock to update mock :

        re run make test : 
            #  to make sure everything is run as expected

        add new test for new session.sql.go :
      
    - modify login gin  api to create, store the refresh token in db and return the refresh token together with access token :

        - update api test function to addapt new modification :

    - add new api to renew the access token when it expires but refresh token not expires yet (token.go) :

    - new renew access token api handler as a new route in the server.go file :

  
  
[39] intro to GRPC
    #  When it comes to performance, HTTP JSON APIs are no match for gRPC

    #  Why GRPC

    # 1. Automatic code generation
        #  Codes that serialize / deserialize data, or transfer data between client & server are automatically generated

    # 2. Strong API Contract
        # Server & client share the same protobuf RPC definition with strongly typed data

    # 3. High performance
        # HTTP/2 : binary framing, multiplexing, header comporession, bidirectional communication

    #  GRPC GATEWAY
    #  server both GRPC and HTTP request at once
        # - A plugin of protobuf compiler 
        # - Generate HTTP proxy codes from protobuf 
        # - Trnaslate HTTP JSON calls to gRPC 
        #     - in-process translation : only for unary 
        #     - separate proxy server : both unary and streaming 
        # - write code once, serve both gRPC and HTTP request


[40] Define gRPC API and generate Go code with protobuf
  #  https://grpc.io/docs/languages/go/quickstart/
    - install protobuf compiler : 
        $ apt install -y protobuf-compiler
        $ protoc --version  # Ensure compiler version is 3+
    
    - to generate golang code, we have to install go plugin for the compiler :
        $ go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28
        $ go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.2

        to check it :
            protoc-gen-go --version
            protoc-gen-go-grpc --version
 
    - write protbuf definition for api (inside proto) : 
      # Protocol buffer documentation: https://protobuf.dev/programming-guides/proto3/
      - defined user object using protobufer :
        # its like a custom data type

    - add to setting.json in vscode to setting path:
        "protoc": {
            "options": [
                "--proto_path=proto",
            ]
        },


    - define object request and response for RPC (create user API) :

    - declare a gRPC service and add the RPC definition of that API:

    - generating Golang codes from service definition that we already create before : 

      -  add to makefile and run : 
          $ protoc --go_out=. --go_opt=paths=source_relative \
          --go-grpc_out=. --go-grpc_opt=paths=source_relative \
          helloworld/helloworld.proto

      - run go mod tidy to fix all packages problem in generated code :

[41] run a golang gRPC server and call it API

    - create gapi folder :

    - setup server :
        # same as http api server. the difference is we're gonna serve gRPC request instead of http

    - start the server to listen to gRPC requests on a specific port :
        - update app.env and config util to add new GPRC server address :


    - update main.go to start GRPC server instead of HTTP server :

    - start the server :
        run :
            make server
            

    - install evans, grpc client cli tool :

        - install locally :
            go install github.com/ktr0731/evans@latest
        
        # prefer menggunakan docker 
        - add in make file :
            evans --host localhost --port 9090 -r repl

    - evans commands :
        show service
        call CreateUser

[42] Implement gRPC API to create and login users in Go
    # it would be very similar to what we've implemented before for gin api

    - define each RPC function in SimpleBankServer interface  : 

[43] grpc GateWay. Write code once, serve both gRPC & HTTP requests.

    - install the build tools for GRPC Gateway :
        go get github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-grpc-gateway
        go get github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-openapiv2
        go get google.golang.org/grpc/cmd/protoc-gen-go-grpc

        go install \
          github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-grpc-gateway \
          github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-openapiv2 \
          google.golang.org/protobuf/cmd/protoc-gen-go \
          google.golang.org/grpc/cmd/protoc-gen-go-grpc

    - download these and :
        https://github.com/googleapis/googleapis.git
        
        - download and unzip :

        - mkdir -p google/api folder inside proto folder :

        - go to google/api folder :

        - copy all this to the google/api :
            google/api/annotations.proto
            google/api/field_behavior.proto
            google/api/http.proto
            google/api/httpbody.proto    


    - add the option google.api.http inside the body of the RPC definition in service in proto :
      #  where we can customize the method, the path, and the body

    - update protoc  command in makefile so that it will generate both the stubs for gRPC as well as the gateway :

          rm -f pb/*.go
        protoc --proto_path=proto --go_out=pb --go_opt=paths=source_relative \
                --go-grpc_out=pb --go-grpc_opt=paths=source_relative \
                proto/*.proto

          to


        rm -f pb/*.go
        protoc -I . --grpc-gateway_out=pb ./gen/go \ 
        --grpc-gateway_opt paths=source_relative \
        --proto_path=proto --go_out=pb --go_opt=paths=source_relative \
                --go-grpc_out=pb --go-grpc_opt=paths=source_relative \
                proto/*.proto

    - setup and run HTTP gateway in main.go :

[44] extract info from gRPC metadata 
    # mengekstrak metadata dari grpc
    # metadata is some information about particular RPC call.
    # in form of list key - value pairs.
    # let client provide extra information assosiated with the call to the server and vice versa.

    # kita akan menggunakan metadata untuk mengakses user agent dan IP address dari client

    # jika kita menggunakan Gateway yang mana akan menghasilkan server GRPC dan HTTP maka metadata bisa saja di store dalam format yang berbeda

    -  setup metadata.go :

    - update loginUser gapi handler to return UserAgent and ClientIP : 

[47] Validate gRPC parameters and send human/machine friendly response

    - create separate package for input data validation to keep gapi package stay clean:

    - add error.go in gapi to store grpc server errors and refactor some function :

    - implement valitador to gapi rpcs to check eacch request field :

[49] Partial update DB record with SQLC nullable parameters

    - add new SQL query function to update user this query implement partial update:
    # this query are implement nullable parameter.
    # for more detail : https://docs.sqlc.dev/en/latest/howto/named_parameters.html#nullable-parameters

    # coalesce(first,sec) = adalah function dari postgres yang akan mengembalikan nilai parameter pertama jika ada (non null), jika tidak ada kembalikan nilai kedua

    # hashed_password = coalesce(sqlc.narg(hashed_password), hashed_password)
    # argument pertama adalah nilai baru yang ingin kita input. sqlc.narg memberi custom nama dan juga nullable pada argument tersebut yagn berarti argument tersebut boleh untuk tidak diisi
    # argument kedua adalah nilai semula dari hashed_password itu sendiri

    # sqlc.arg = memberi nama argument yang wajib diisi
    # sqlc.narg = memberi nama argument yang tidak wajib diisi


        use : 
            -- name: UpdateUser :one
              UPDATE users
              SET
                hashed_password = coalesce(sqlc.narg(hashed_password), hashed_password),
                password_changed_at = coalesce(sqlc.narg(password_changed_at), password_changed_at),
                full_name = coalesce(sqlc.narg(full_name), full_name),
                email = coalesce(sqlc.narg(email), email)
              WHERE
                username = sqlc.arg(username)
              RETURNING *;

    - run make sqlc :
    #  untuk generate kode sqlc yang menggunakan sqlc.narg memerlukan sqlc versi terbaru. jadi pastikan sudah menggunakan versi yang terbaru

    - run make mock :

    - write a new unit test for new update user function in user.sql.go and also to learn how to use it:

[50] Build gRPC update API with optional parameters
    # to build update user API

  - make sure protoc version is at least 3.15 :
  
  - write proto file that use optional keyword for this new API :
    # optional tell protoc that this field is not mandatory

      # message UpdateUserRequest {
      # string username = 1;
      # optional string full_name = 2; // optional tell protoc that this field isnot mandatory to fill
      # optional string email = 3;
      # optional string password = 4;
      # }

  - add a new RPC to simple bank service :
      add google api http to add it to gateway server generated code

  - add to make proto script :
      --experimental_allow_proto3_optional


  - run make proto to generate golang code :

  - implement update User GAPI in our GRPC server use new generated code :
  
